{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Landing using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Vishal Shetty, November 5, 2021* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the machine learning concepts, algorithms, and data that interest you.  Describe why you are interested in these:\n",
    "\n",
    "1: The concept of reinforcement learning piqued my interest for its applications in control theory, when dealing with uncertain conditions.\n",
    "\n",
    "2: The recent strides made in reinforcement learning make it relatively easy to apply learning based control strategies towards applications in complex systems, where we as humans are not able to see underlying patterns which lead to an optimal solution.\n",
    "\n",
    "Very briefy describe your planned methods:\n",
    "\n",
    "1: Use the enviornments provided by OpenAI on their platform, to train and test the Temporal Difference-TD(0) with Q-learning algorithm implemented in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in some detail the algorithms and data you will use:\n",
    "\n",
    "1: The LunarLander-v2 enviornment will be used from the OpenAI gym to train and test our reinforcement learning agent.\n",
    "\n",
    "2: Below cells describe the details about the enviornment : States , Actions, Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4) Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32) (8,)\n"
     ]
    }
   ],
   "source": [
    "## Citation[1]\n",
    "import gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "states = env.observation_space ## The observations provided by the enviornment\n",
    "actions = env.action_space     ## The action space for the agent to select its actions from\n",
    "print(actions,states,states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space:   \n",
    "\n",
    "Discrete (Discrete Action Space with 4 values):\n",
    "\n",
    "0 = Do Nothing\n",
    "\n",
    "1 = Fire Left Engine\n",
    "\n",
    "2 = Fire Main Engine\n",
    "\n",
    "3 = Fire Right Engine [2]\n",
    "\n",
    "### Observation Space:\n",
    "The observation space for the Lunar Lander is illustrated by a \"Box\" containing 8 values between [ $-\\infty$, $\\infty$ ] these values are:\n",
    "\n",
    "Position X\n",
    "\n",
    "Position Y\n",
    "\n",
    "Velocity X\n",
    "\n",
    "Velocity Y\n",
    "\n",
    "Angle\n",
    "\n",
    "Angular Velocity\n",
    "\n",
    "Is left leg touching the ground: 0 OR 1\n",
    "\n",
    "Is right leg touching the ground: 0 OR 1 [2]\n",
    "\n",
    "### Reward System:\n",
    "\n",
    "Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite.[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a test run of the lunar lander without any training based on random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Score:-382.76200053806633\n",
      "Episode:1 Score:-64.8373431659697\n",
      "Episode:2 Score:-137.30806286788524\n",
      "Episode:3 Score:-59.221472281021676\n",
      "Episode:4 Score:-186.10538350627127\n",
      "Episode:5 Score:-114.52278555700988\n",
      "Episode:6 Score:-113.16441953486438\n",
      "Episode:7 Score:-351.54791324452214\n",
      "Episode:8 Score:-244.82510361301874\n",
      "Episode:9 Score:-120.8438457367883\n",
      "Episode:10 Score:-139.68522829057528\n"
     ]
    }
   ],
   "source": [
    "## Citation[1]\n",
    "##To run the below code you would need below packages\n",
    "##pip install gym\n",
    "##conda install swig\n",
    "##pip install box2d\n",
    "##pip install pyglet\n",
    "import gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "for i_episode in range(11):\n",
    "    observation = env.reset()#state initailized to a random point\n",
    "    done = False # information on terminal state\n",
    "    score = 0 #initializing reward\n",
    "    while not done:\n",
    "        env.render()\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample() ##random action selection without any training\n",
    "        next_observation, reward, done, info = env.step(action)##nextstate_reward_terminalstate\n",
    "        score +=reward ##reward_update_step\n",
    "    print('Episode:{} Score:{}'.format(i_episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the rewards accumulate to give a negative score in all 10 episodes with random action selection.\n",
    "Our goal with the RL agent implementation would be to get a high positive accumulated reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms to be employed:\n",
    "#### Temporal Difference-TD(0) :\n",
    "Temporal difference learning can be defined as a chain of predictions, it does not calculate total future reward, TD simply tries to predict the combination of immediate reward and its own reward prediction at the next moment in time.Then, when the next moment comes, bearing new information, the new prediction is compared against what it was expected to be. If they’re different, the algorithm calculates how different they are, and uses this “temporal difference” to adjust the old prediction toward the new prediction. [ 4 ]\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "value(st,at) ≈ rt+1 + value(st+1,at+1)  [ 5]\n",
    "\n",
    "#### Q-Learning :\n",
    "The objective for any reinforcement learning problem is to find the sequence of actions that maximizes (or minimizes) the sum of reinforcements along the sequence. This is reduced to the objective of acquiring the Q function that predicts the expected sum of future reinforcements, because the correct Q function determines the optimal next action. [ 6 ]\n",
    "\n",
    "Mathematically : \n",
    "\n",
    "Q(st,at) ≈  ∑k=0∞ (rt+k+1)         [6]                  \n",
    "\n",
    "#### Policy : Epsilon-greedy :\n",
    "Epsilon (ϵ) indicates the probability of taking a random action. Its value is from 0 to 1. Given a Qnet, the current state, the set of valid_actions and epsilon, we can define a function that either returns a random choice from valid_actions or the best action determined by the values of Q produced by Qnet for the current state and all valid_actions. This is referred to as an ϵ-greedy policy. [ 6 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In your proposal, make a table here with at least 5 milestones for your project with expected dates: \n",
    "\n",
    "1: 6th November 2021 to 10th November 2021 : Setting up OpenAI enviornment.\n",
    "\n",
    "2: 11th November 2021 to 20th November 2021 : Implementation of RL agent on the OpenAI enviornment\n",
    "\n",
    "3: 21st November 2021 to 25th November 2021 : Tuning hyperparameters for optimal performance.\n",
    "\n",
    "4: 26th November 2021 to 5th December 2021 : Analysis based on the performance of the RL agent\n",
    "\n",
    "3: 6th December 2021 to 10th December 2021 : Presentation of results and demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the proposal, summarize what you expect your results to be:\n",
    "\n",
    "1: Since there is infinite fuel, the RL agent would learn to hover/fly first.[2]\n",
    "\n",
    "2: Achieving a positive score value (that is positive accumulated reward) for each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your proposal, describe what you expect to learn:\n",
    "\n",
    "1: Behavior of RL agent under different hyperparameters\n",
    "\n",
    "What you expect will be most difficult:\n",
    "\n",
    "1: Implementing the RL agent on the OpenAI enviornment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OpenAI gym enviornment documentation : https://gym.openai.com/docs/#environments .\n",
    "* OpenAI gym enviornment - LunarLander - github : https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "* OpenAI gym enviornment - LunarLander : https://gym.openai.com/envs/LunarLander-v2/ \n",
    "* Temporal Difference : https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI\n",
    "* Lecture 17 - CS545: https://nbviewer.org/url/www.cs.colostate.edu/~anderson/cs545/notebooks/15%20Introduction%20to%20Reinforcement%20Learning.ipynb\n",
    "* Lecture 18 - CS545:\n",
    "https://nbviewer.org/url/www.cs.colostate.edu/~anderson/cs545/notebooks/16%20Reinforcement%20Learning%20with%20Neural%20Network%20as%20Q%20Function.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T14:49:15.869360Z",
     "start_time": "2021-10-27T14:49:15.860735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Shetty-ProjectProposal.ipynb is 822\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "nbfile = glob.glob('Shetty-ProjectProposal.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
